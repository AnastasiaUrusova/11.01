# 
1. Порядок выполнения предобработки и разведочного анализа данных
Удаление дубликатов 
Дублирующие записи не только искажают статистические показатели датасета, но и снижают качество обучения модели, потому удалим полные дублирующие вхождения. Для начала уточним, сколько записей в датасете с помощью свойства Pandas.DataFrame.shape:
Удалим дублирующие записи с помощью Pandas.drop_duplicates() и обновим данные о размере данных:
```
df = df.drop_duplicates()
df.shape
```
Обработка пропусков
Стоит помнить, что в случае, если пропусков у признака слишком много (более 70%), такой признак удаляют. Проверим, насколько полны наши признаки: метод isnull() пройдется по каждой ячейке каждого столбца и определит, кто пуст, а кто нет, составив датафрейм такого же размера, состоящий из True / False. Метод mean() суммирует все значения True, определит концентрацию пропусков в каждом столбце. На 100 мы умножаем, чтобы получить значение в процентах:
```
df.isnull().mean()*100
```
Таким образом, переменная подлежит удалению с помощью drop():
```
df=df.drop(columns=['День'])
```
Существует несколько способов обозначить пропуски, и зачастую создатели датасета не описывают данные в достаточной мере, и определять, как обозначены пропуски, приходится вручную. Из встреченных доселе 
обозначений приведу следующие:
 NaN / NaT (упрощенно: "не число" / "не время")
 Пустая ячейка
 Для числовых признаков – радикальный выброс. К примеру, для столбца 
"День" это число 999.
 Маркер или нестандартный символ
Встроенные методы Pandas позволяют с легкостью справиться с первыми двумя разновидностями таких пробелов. Разберемся для начала с категориальными переменными, объединив их в один вектор. Список получится совсем уж нелогичный, но это не столь важно в данной ситуации: мы лишь ищем способы обозначения пропуска
Процесс обработки пропусков, к счастью, можно сократить с помощью sklearn.impute.SimpleImputer. Мы выбираем все категориальные переменные и применяем стратегию "[вставить вместо пропуска] самое распространенное значение".
Описательная статистика
Признаки, принадлежащие к булевому типу данных, обрабатываются алгоритмом тем же образом. Целевую переменную Y мы не обрабатываем (если в этом столбце есть пропуски, такие строки стоит удалить)
Обнаружение аномалий
Самый легкий способ обнаружить выбросы – визуальный. Мы построим разновидность графика "ящик с усами" для одной из числовых переменных – "Возраст":
```
df.boxplot(colum=['возраст'],figsize=(2,7))
```
![image](https://user-images.githubusercontent.com/112615007/211721574-3c3ef621-c27e-49e2-8460-c898c26f6159.png)

Описательная статистика:
Одномерный анализ
Описательная статистика
Прежде чем применять те или иные методы обучения, нам необходимо удостовериться, что они применимы к текущему датасету. Раздел описательной статистики включает в себя проверку на нормальность распределения и определение прочих статистических метрик. С этим нам поможет замечательная библиотека pandas-profiling. Установим самую вежую версию во избежание ошибок:
Следующий интересный раздел – "Корреляции" ('Correlations'). Чем ярче (краснее / синее) ячейка, тем сильнее выражена корреляция между парой признаков. Диагональные ячейки игнорируются, поскольку являются результатом расчета коэффициента между переменной и ее копией

Профайлер вычленил из датасета только числовые признаки, и потому матрица имеет размер 11 x 11. К примеру, "колебание уровня безработицы" и "европейская межбанковская ставка" сильно коррелируют друг с другом, но поскольку эти признаки второстепенны, в дальнейшем их можно объединить на этапе инжиниринга признаков (Feature Engineering). Зачастую целевая переменная не сильно коррелирует с предикторами

Важность признаков
Прежде чем произвести инжиниринг признаков и сократить объем входных данных, стоит определить, какие признаки имеют первостепенную значимость, и в этом нам поможет Scikit-Learn и критерий Хи-квадрат (ChiSquared Test).:
![image](https://user-images.githubusercontent.com/112615007/211724154-026b5123-1344-4225-b313-5e2fc0d2f4ab.png)

Уменьшение размерности, стандартизация
Рассмотрев признаки по отдельности и попарно, мы пришли к выводу, что некоторые признаки могут быть как бы объединены с помощью специальной техники – Анализ главных компонент (PCA). Итак, давайте создадим заменяющий столбец, который представляет эти признаки в равной мере и тем самым уменьшим размер данных.

![image](https://user-images.githubusercontent.com/112615007/211724444-77dcfdfe-6029-4eef-b05f-5c4f2a9a5944.png)

![image](https://user-images.githubusercontent.com/112615007/211724482-2bcfcb2c-b9a9-4ce3-ab58-4bfeb293771d.png)

Анализ главных компонент (Principal Component Analysis) представляет собой метод уменьшения размерности больших наборов данных путем преобразования большого набора переменных в меньший с минимальными потерями информативности.

![image](https://user-images.githubusercontent.com/112615007/211724588-b45ef141-a4e7-4129-ac21-ca4a276dee26.png)

Мы получили два принципиальных компонента и путем такого сокращения понижаем размерность датасета без потерь.

![image](https://user-images.githubusercontent.com/112615007/211724645-f7f8893d-6c3b-44f5-8133-e81d864fb8d0.png)

Нормализация
Еще один шаг, не затронутый в примере выше, – это нормализация и порой приходится выбирать между ею и стандартизацией. Мы нормализуем те же признаки, характеризующие состояние экономики и потому загрузим датасет в исходном виде еще раз

![image](https://user-images.githubusercontent.com/112615007/211724759-309be608-4de0-43b2-bbdf-972e68bd6c07.png)


Алгоритмы кодирования категориальных признаков. В каком случае какой алгоритм применять;
Есть много способов, которыми мы можем кодировать эти категориальные переменные как числа и использовать их в алгоритме. 

Одна горячая кодировка
В этом методе мы сопоставляем каждую категорию с вектором, который содержит 1 и 0, обозначая наличие или отсутствие функции. Количество векторов зависит от количества категорий для объекта. Этот метод создает множество столбцов, которые значительно замедляют процесс обучения, если число категорий для функции очень велико. Панды имеетget_dummiesфункция, которая довольно проста в использовании. Для примера код фрейма данных будет выглядеть так:
```
df=pd.get_dummies(df,prefix=['Temp'], columns=['Temperature'])
df
```
![image](https://user-images.githubusercontent.com/112615007/211722248-4fd5a70c-dfcf-41c2-afb3-8f669a50faa3.png)

Scikit-учить естьOneHotEncoderдля этой цели, но он не создает столбец дополнительных функций (требуется дополнительный код, как показано в примере кода ниже).

![image](https://user-images.githubusercontent.com/112615007/211722350-55cbc9ea-9e23-406e-9f76-0461d322c78a.png)

One Hot Encoding очень популярен. Мы можем представить всю категорию с помощью N-1 (N = No of Category), поскольку этого достаточно для кодирования того, что не включено. Обычно для регрессии мы используем N-1 (опустить первый или последний столбец новой функции One Hot Coded), но для классификации рекомендуется использовать все N столбцов без, так как большая часть алгоритма на основе дерева строит дерево на основе всех доступных

![image](https://user-images.githubusercontent.com/112615007/211723959-2c278a50-9aec-43c5-b234-335ff3c5e88f.png)


Label Encoder
Первое (выбранное каким-то образом) уникальное значение кодируется нулем, второе единицей, и так далее, последнее кодируется числом, равным количеству уникальных значений минус единица.

Данный тип кодирования является наиболее часто используемым, преобразование представляет собой однозначное соответствие число <-> уникальное значение категориального признака.
Давайте посмотрим на примере:

Предположим у нас есть категориальный признак бренда автомобиля, со значениями BMW, Mercedes, Nissan, Infinity, Audi, Volvo, Skoda.
Создадим искусственный признак brand и закодируем его с помощью реализации sklearn.
```
data = pd.DataFrame()
data['brand'] = ['BMW', 'Mercedes', 'Nissan', 'Infinity', 'Audi', 'Volvo', 'Skoda']*5
```
Для чистоты эксперимента перемешаем наши данные с помощью метода sample(), зафиксируем random_state для воспроизводимости и посмотрим на шапку данных:
```
data = data.sample(frac=1,random_state=42).reset_index(drop=True)
data.head(10)
```
Применим Label Encoder:
```
from sklearn.preprocessing import LabelEncoder
labelencoder = LabelEncoder()
data_new = labelencoder.fit_transform(data.values)
data_new[:10]
```
итог; array([6,5,2,1,3,3,6,6,3,4])

Посмотрим на "правило преобразования" с помощью метода classes_
```
labelencoder.classes_
```
![image](https://user-images.githubusercontent.com/112615007/211722933-261255d7-99c5-46e4-b2fe-c450a9dd4616.png)

Реализация Label Encoder в sklearn прежде всего сортирует по алфавиту уникальные значения, потом присваивает им порядковый номер!


Алгоритм построения модели линейной регрессии
Регрессия — способ выбрать из семейства функций ту, которая минимизирует функцию потерь. Последняя характеризует насколько сильно пробная функция отклоняется от значений в заданных точках.

Цель регрессии — найти коэффициенты этой линейной комбинации, и тем самым определить регрессионную функцию $f$ (которую также называют моделью). Отмечу, что линейную регрессию называют линейной именно из-за линейной комбинации базисных функций — это не связано с самыми базисными функциями (они могут быть линейными или нет).

![image](https://user-images.githubusercontent.com/112615007/211726071-2d9dedb3-da63-4ae0-a079-71dacb0e5198.png)

Метод наименьших квадратов

Начнём с простейшего двумерного случая. Пусть нам даны точки на плоскости {(x_1,y_1),(x_N,y_N) и мы ищем такую аффинную функцию
f(x) = a + b*x

![image](https://user-images.githubusercontent.com/112615007/211726462-b782cf5b-e477-4ff6-a070-41b427e318fe.png)

Как видно из иллюстрации, расстояние от точки до прямой можно понимать по-разному, например геометрически — это длина перпендикуляра. Однако в контексте нашей задачи нам нужно функциональное расстояние, а не геометрическое. Нас интересует разница между экспериментальным значением и предсказанием модели для каждого xi поэтому измерять нужно вдоль оси y

![image](https://user-images.githubusercontent.com/112615007/211726574-0978af48-7744-4f92-b1fc-544a203fda09.png)

Метод наименьших квадратов (по англ. OLS) — линейная регрессия c SSE(a,b) в качестве функции потерь.

![image](https://user-images.githubusercontent.com/112615007/211726654-ae37e36c-45ed-4490-89e7-a2eb2034634d.png)

# Алгоритм построения моделей кластеризации.

![image](https://user-images.githubusercontent.com/112615007/211727274-7c7ec5e5-aebc-405f-a5cc-e523114bda2e.png)

Иерархический кластерный анализ

Каждое наблюдение образовывает сначала свой отдельный кластер.
• На первом шаге анализа два соседних кластера объединяются в один.
• Этот процесс продолжается до тех пор, пока не останутся только два кластера. 
• Расстояние между кластерами является средним значением всех расстояний между всеми возможными парами точек из обоих кластеров (Between-groups linkage (Связь между группами))

Иерархическая кластеризация – это альтернативный подход, который не требует выбора К. Результат иерархической кластеризации представляется в древообразном представлении наблюдений, называемом дендрограммой.
![image](https://user-images.githubusercontent.com/112615007/211728841-24c82894-1261-444b-9b3d-837a47c97594.png)
Разрежем дендрограмму по горизонтали, как это показано в центре и справа на рис. 1. Отдельные 
множества наблюдений, которые находятся снизу от разреза могут рассматриваться в качестве кластеров. В центре рис. 1 разрез дендрограммы на высоте 9 дает 2 кластера, показанных разными цветами. Справа разрез дендрограммы на высоте 5 дает 3 кластера. Можно сделать и другие разрезы дендрограммы. 1 кластер получится, если не разрезать, а на высоте 0 будет n кластеров, т.е. каждое измерение будет отдельным кластером. Другими словами, высота разреза дендрограммы играет ту же роль, что и К в кластеризации К-средних: контролирует число полученных кластеров


Алгоритм k-means (k-средних)
Шаг 1. Вначале возьмем данные и самостоятельно выберем желаемое количество кластеров и обозначим их буквой k (отсюда название метода). Пусть в данном случае их будет три.

![image](https://user-images.githubusercontent.com/112615007/211728033-0806b7e8-46ce-4165-a2ee-6d80136de742.png)

Шаг 2. Расположим несколько точек. Их количество будет равно количеству кластеров. Эти точки называются центроидами. Посчитаем расстояние от наших данных до каждого из центроидов. Логично отнести наблюдение к тому центроиду, который находится ближе.

![image](https://user-images.githubusercontent.com/112615007/211728069-90d0c081-12d5-4ee6-8e76-b0f1735406c5.png)

Шаг 3. Таким образом, каждая точка будет отнесена к определенному центроиду (кластеру).

![image](https://user-images.githubusercontent.com/112615007/211728122-d0ebbdd8-f156-4955-948a-970aab56ca74.png)

Шаг 4. Сместим наши центроиды в центр получившихся кластеров.

![image](https://user-images.githubusercontent.com/112615007/211728157-a07b2eea-cf65-4197-b9ae-4e64a8c5983a.png)

Шаг 5. Вновь отнесем точки к каждому из центроидов. Некоторые наблюдения «переметнутся» к другому центроиду.

![image](https://user-images.githubusercontent.com/112615007/211728202-9d61802f-b877-418c-a68b-92cc2e0c2351.png)

Мы будем повторять шаги 4 и 5 до тех пор, пока алгоритм не стабилизируется, то есть до тех пор, пока наблюдения не перестанут переходить от одного центроида (кластера) к другому.
Сколько кластеров выбрать?
Есть два способа выбора количества кластеров:

Экспертный метод. Выбор количества кластеров будет зависеть от знания о предметной области (domain knowledge)
Метод локтя (elbow method). Мы также можем (1) обучить модель используя несколько вариантов количества кластеров, (2) измерить сумму квадратов внутрикластерных расстояний и (3) выбрать тот вариант, при котором данное расстояние перестанет существенно уменьшаться.
На графике метод локтя выглядит следующим образом.

![image](https://user-images.githubusercontent.com/112615007/211728305-9e1be329-ff51-4e87-ab2e-accaa4b43804.png)

Как мы видим, после того как количество кластеров достигает трех, сумма квадратов внутрикластерных расстояний перестает существенно уменьшаться. Значит в данном случае три кластера и будет оптимальным значением.






